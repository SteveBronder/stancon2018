---
title: "GPU Optimized Math Routines in the Stan Math Library"
author: |
  | Rok Češnovar, Davor Sluga, Jure Demšar, Steve Bronder, Erik Štrumbelj
date: "Apr 16, 2018"
output: pdf_document
bibliography: report.bib
---


```{r include = FALSE}
# wd
#setwd("D:/Projects/StanCon2018")
#setwd("d:/work/bitbucket/l1-7542_stan/StanCon2018/")

# includes
library(ggplot2)
library(plyr)
library(rstan)
library(cowplot)
library(reshape)
```

# Introduction

The Stan Math library's Hamilton Monte Carlo (HMC) sampler has computationally expensive draws while usually searching the target distribution more efficiently than alternative MCMC methods with fewer iterations. The bottleneck within draws makes Stan a prime candidate for GPU optimizations within samples. This project implements GPU optimizations for the Cholesky decomposition, and it's derivative, in the Stan Math library [@stanmath2015]. Several modeling languages exist which perform MCMC sampling using GPUs. However, this is the first known open source implementation of the Cholesky decomposition with a GPU in an HMC setting. Furthermore, the GPU kernels use OpenCL which allows the use of these methods across any brand of GPU. While results show that GPU optimizations are not optimal for small $N\times M$ matrices, large matrices can see speedups of 13 fold while retaining the same precision as models run purely on a CPU. 

# GPU implementation

<!--
Currently, we aim at speeding up the biggest computational bottlenecks on the GPU, while the remaining (non-parallelized) Stan code is executed on the CPU. Therefore, we move the data to and from the GPU for each GPU-parallelized function. Removing this often unnecessary data transfer to and from the GPU is one of the main priorities of  future work.
-->
The most significant linear algebra bottlenecks in stan tend to come from the Cholesky Decomposition, it's derivative, and the derivative of solving $Ax = B$. To reduce these bottlenecks, the GPU methods implemented in the Stan Math library include:

\begin{enumerate}
\item	Matrix transpose
\item	Multiplication of matrices with a diagonal and scalar
\item	Subtraction of matrices
\item	Copying submatrices
\item	Matrix multiplication
\item	Lower triangular matrix inverse
\item	Cholesky decomposition
\item First derivative of Cholesky Decomposition
\end{enumerate}

The execution times of methods (1-4) are negligible and thus our GPU implementations of these methods are simple and naive. For instance, in multiplication of a $m \times n$ matrix with a scalar we create $m \times n$ threads, where each thread is assigned a single multiplication. These implementations are necessary to implement methods (6-8) on the GPU.

The routines in cuBLAS [@CUBLAS] and clBLAST are the basis of stan's matrix multiplication routines. The matrix multiplication routines are optimized through two standard methods, assigning additional work to threads in large matrix multiplications and the use of tiling in local memory. Optimizations for specific cases are created such as $A \times A^T$. Because the result is symmetric, the routine will reduce the number of multiplications needed by one half. 

The GPU implementations and optimizations of the lower triangular matrix inverse and the Cholesky decomposition are improvements on our previous work in [@Cesnovar2017]. Details of these implementations are available in the next sections. The first derivative of the Cholesky decomposition is implemented using methods (1-7).

The OpenCL [@StoneOpenCL2010] context which manages the devices, platforms, memory, and kernels sits in \texttt{opencl\_context\_base::getInstance()} and is built into stan using the C++11 Meyer's singleton pattern. Developers can access the context through a friend adapter class called \texttt{opencl\_context} which provides a simple wrapper API for accessing the base context.

## Inverting a lower triangular matrix

The most widely used CPU algorithms for inverting a lower triangular matrix are not suitable for many-core architectures. Figure \ref{fig:blockInverse} gives a graphical illustration of the solution proposed in [@Mahfoudhi2012] which replaces most of the sequential code with matrix multiplications and is more suited for many-core systems.

The input matrix is split into blocks as shown in Figure \ref{fig:blockInverse}. The first step is to calculate the matrix inversion of the smaller matrices $A1$ and $A2$. These inverses are done using the basic sequential algorithms, with small amounts of parallelism. The final step is the calculation of $C3 = -C2 \times A3 \times C1$. The optimal number of blocks depends on the input matrix size, and the GPU used\footnote{ Thread blocks and warps will be in groupings of powers of two, so the optimal block size is recommended to be a power of two such as 32x32}. 

![\label{fig:blockInverse}Blocked version of the lower triangualar matrix inverse.](_Figures/blockInverse.pdf)

## Cholesky decompostion

The GPU implementation of the Cholesky Decomposition comes from the blocked algorithm proposed in [@LouterNool1992]. Similar to the application of the lower triangular matrix inverse, the input matrix is split into blocks, as shown in Figure \ref{fig:blockCholesky}. A basic algorithm is first used to calculate the Cholesky Decomposition of $A_{11}$ and then the calculation of the inverse of $L_{11}^T$. $L_{21}$ and $L_{22}$ proceeds as follows:

$$L_{21} = A_{21} (L_{11}^T)^{(-1)}$$

$$L_{22} = A_{22} - L_{21} (L_{21})^T$$

For larger matrices $(n > 1000)$, the algorithm is executed in 2 levels. For example, when $n = 2000$, the size of the block $A_{11}$ is $m = 400$. Because the sequential algorithm would be slow for a large $A_{11}$ block, the routine is run recursively on $A_{11}$ until $m$ reaches a reasonable size. 

![\label{fig:blockCholesky}Blocked version of the Cholesky decomposition.](_Figures/blockCholesky.pdf)

The implementation of the derivative of the Cholesky decomposition comes from the blocking method presented in [@Murray2016]. This algorithm is cache friendly and uses GPU-suitable matrix operations. Similar to the inversion and Cholesky Decomposition, the input matrix splits into smaller blocks on which we then perform various matrix operations: transpose, multiplication, lower triangular matrix inversion and subtraction. For details on the algorithm, refer to [@Murray2016].

Users can access the Cholesky GPU routines by calling \texttt{cholesky\_decompose\_gpu()} and \texttt{multi\_normal\_cholesky\_gpu()} in the stan language. In the latter, only the derivative of solving $Ax=b$ is run on the gPU. In the future, all GPU methods will be implemented in the same way so that users can make their code access the GPU routines by calling \texttt{<func\_name>\_gpu()}.

# Example 1: Gaussian process regression

Models that use large covariance matrices will tend to benefit the most from the Cholesky GPU routines. The example below uses 1D GP regression with hyperpriors as exampled in the case study [@Betancourt2017] (see appendix).

This example uses a toy dataset based on a simple, functional relationship between $x$ and $y$ with added Gaussian noise:

$$x_i \sim_{\text{iid}} U(-10,10)$$
$$y_i | x_i \sim_{\text{iid}} N \left( f(x), \frac{1}{10} \right), i = 1..n,$$
where $f(x) = \beta(x + x^2 - x^3 + 100 \sin 2x - \alpha)$. Parameters $\beta$ and $\alpha$ were set so that $E[f] = 0$ and $Var[f] = 1$. Figure \ref{fig:GP_fit} shows that there is no difference between GPU and CPU fits.

```{r echo = FALSE, fig.cap="\\label{fig:GP_fit}Comparison of CPU and GPU fits.", fig.width = 12, fig.height = 6}

# load  results ----
stan_files <- list.files(
  path = "./_Results/",
  pattern = "GP", full.names = T
)

for (fn in stan_files) {
  summary <- readRDS(fn)
  
  # take one fit on largest N ----
  if (summary$N == 1024 && summary$iteration == 1)
  {

    if (summary$GPU)
      gpu_predict <- data.frame(x = summary$x,
                                y = summary$y,
                                x_predict = summary$x_predict,
                                y_predict = summary$y_predict,
                                GPU = TRUE)
    else
      cpu_predict <- data.frame(x = summary$x,
                                y = summary$y,
                                x_predict = summary$x_predict,
                                y_predict = summary$y_predict,
                                GPU = FALSE)
  }
}

df <- rbind(gpu_predict, cpu_predict)

ggplot() +
  geom_point(data = gpu_predict, aes(x = x, y = y), alpha = 0.1, shape = 16) +
  geom_line(data = df, aes(x = x_predict, y = y_predict, colour = GPU), size = 1) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_colour_manual(values = c("#fc8d59", "#91bfdb"), labels = c("CPU", "GPU"))

```

We ran the model for multiple input sizes for both the CPU and GPU. The measured times include sampling and warmup iterations, but not model compilation time\footnote{ Hardware for these experiments includes a desktop computer with an Intel Core i7-4790 CPU running at $3.6 GHz$ and a Nvidia GTX1070 GPU}. Results are shown in Figure \ref{fig:GP_results}. Due to unnecessary data transfers, the GPU implementation is not faster than the CPU version for smaller input sizes ($N < 750$). For larger $n$, the data transfer becomes negligible, and we can observe a speedup of $~7.8$ for $N = 5120$. Speedup measurements for larger $N$ were infeasible due to large CPU computation times. 

```{r echo = FALSE, fig.cap="\\label{fig:GP_results}Visualizations of speedup when using our GPU approach compared to the default CPU implementation. For simulations ran with the default CPU implementation we only ran simulations up to $N = 2048$, times reported for larger N's are estimated on measurements for smaller N's.", fig.width = 12, fig.height = 6}

# load  results ----
stan_files <- list.files(
	path = "./_Results/",
	pattern = "GP", full.names = T
)

# load data ----
stan_summary <- NULL
stan_summary_log <- NULL

for (fn in stan_files) {
	summary <- readRDS(fn)
	
	stan_summary <- rbind(stan_summary, data.frame(N = summary$N,
																								 time = summary$time,
																								 GPU = summary$GPU))
	
	stan_summary_log <- rbind(stan_summary_log, data.frame(N = log(summary$N, 10),
																												 time = log(summary$time, 10),
																												 GPU = summary$GPU))
}

# interpolate CPU ----
df_cpu <- stan_summary_log[stan_summary_log$GPU == FALSE, ]
lm_fit <- lm(time ~ poly(N,2), data = df_cpu)

N <- c(log(3072, 10), log(4096, 10), log(5120, 10))
df_interpolate <- data.frame(N = N,
                             time = predict(lm_fit, newdata = data.frame(N = N)),
                             GPU = FALSE)

stan_summary_log <- rbind(stan_summary_log, df_interpolate)

df_interpolate$N = c(3072, 4096, 5120)
df_interpolate$time = 10^df_interpolate$time
stan_summary <- rbind(stan_summary, df_interpolate)

# get mean and CI ----
stan_mean <- ddply(.data = stan_summary,
									 .variables = ~ N + GPU,
									 .fun = summarize,
									 time_mean = mean(time),
									 low_time = quantile(time, 0.025, na.rm = TRUE),
									 high_time = quantile(time, 0.975, na.rm = TRUE))

stan_mean_log <- ddply(.data = stan_summary_log,
											 .variables = ~ N + GPU,
											 .fun = summarize,
											 time_mean = mean(time),
											 low_time = quantile(time, 0.025, na.rm = TRUE),
											 high_time = quantile(time, 0.975, na.rm = TRUE))

# plot ----
left_plot <- ggplot(data = stan_mean,
										aes(x = N, y = time_mean, group = GPU, colour = GPU)) +
	geom_point(data = stan_summary,
						 aes(x = N, y = time, colour = GPU),
						 size = 4, alpha = 0.3, shape = 16) +
	geom_line(size = 1) +
	ylab("Time [s]") +
	xlab("N") +
	theme_minimal() +
	theme(legend.position = "bottom", legend.title = element_blank()) +
	scale_colour_manual(values = c("#fc8d59", "#91bfdb"), labels = c("CPU", "GPU"))

right_plot <- ggplot(data = stan_mean_log, 
										 aes(x = N, y = time_mean, group = GPU, colour = GPU)) +
	geom_point(data = stan_summary_log, aes(x = N, y = time, colour = GPU),
						 size = 4, alpha = 0.3, shape = 16) +
	geom_line(size = 1) +
	ylab(expression(log[10](time)~"[s]")) +
	xlab(expression(log[10](N))) +
	theme_minimal() +
	theme(legend.position = "bottom", legend.title = element_blank()) +
	scale_colour_manual(values = c("#fc8d59", "#91bfdb"), labels = c("CPU", "GPU"))

plot_grid(left_plot, right_plot, ncol = 2, nrow = 1, scale = 0.9)
```

# Conclusion

This project shows the GPU routines in stan can give both practical and powerful speedups. Parallelizing the Cholesky, it's derivative and the derivative of solving $Ax=B$ can provide a $7.8$ fold speedups or more for programs which depend on large covariance matrices. As this project continues, future areas of research include (a) removing unnecessary data transfers to and from the GPU, which is currently our most significant bottleneck, (b) allowing \texttt{rstan} [@rstan2018] to access the GPU methods, and (c) adding GPU-parallel implementations for other computational building blocks such as other matrix methods, density calculations, and generating random variates.

\newpage
# Appendix

## Stan model for Gaussian process regression

```
functions {
  vector gp_pred_rng(real[] x2,
                     vector y1, real[] x1,
                     real alpha, real rho, real sigma, real delta) {
    int N1 = rows(y1);
    int N2 = size(x2);
    vector[N2] f2;
    {
      matrix[N1, N1] K =   cov_exp_quad(x1, alpha, rho)
                         + diag_matrix(rep_vector(square(sigma), N1));
      matrix[N1, N1] L_K = cholesky_decompose(K);

      vector[N1] L_K_div_y1 = mdivide_left_tri_low(L_K, y1);
      vector[N1] K_div_y1 = mdivide_right_tri_low(L_K_div_y1', L_K)';
      matrix[N1, N2] k_x1_x2 = cov_exp_quad(x1, x2, alpha, rho);
      vector[N2] f2_mu = (k_x1_x2' * K_div_y1);
      matrix[N1, N2] v_pred = mdivide_left_tri_low(L_K, k_x1_x2);
      matrix[N2, N2] cov_f2 =   cov_exp_quad(x2, alpha, rho) - v_pred' * v_pred
                              + diag_matrix(rep_vector(delta, N2));
      f2 = multi_normal_rng(f2_mu, cov_f2);
    }
    return f2;
  }
}

data {
  int<lower=1> N;
  real x[N];
  vector[N] y;

  int<lower=1> N_predict;
  real x_predict[N_predict];
}

parameters {
  real<lower=0> rho;
  real<lower=0> alpha;
  real<lower=0> sigma;
}

model {
  matrix[N, N] cov =   cov_exp_quad(x, alpha, rho)
                     + diag_matrix(rep_vector(square(sigma), N));
  matrix[N, N] L_cov = cholesky_decompose(cov);

  // P[rho < 2.0] = 0.01
  // P[rho > 10] = 0.01
  rho ~ inv_gamma(8.91924, 34.5805);
  alpha ~ normal(0, 2);
  sigma ~ normal(0, 1);

  y ~ multi_normal_cholesky(rep_vector(0, N), L_cov);
}

generated quantities {
  vector[N_predict] f_predict = gp_pred_rng(x_predict, y, x, alpha, rho, sigma, 1e-10);
  vector[N_predict] y_predict;
  for (n in 1:N_predict)
    y_predict[n] = normal_rng(f_predict[n], sigma);
}

```
## Original Computing Environment

```{r}
sessionInfo()
```

\newpage
# References