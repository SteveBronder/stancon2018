---
title: "GPU Optimized Math Routines in the Stan Math Library"
author: |
  | Rok Češnovar, Davor Sluga, Jure Demšar, Steve Bronder, Erik Štrumbelj
date: "Apr 15, 2018"
output: pdf_document
bibliography: report.bib
---

```{r include = FALSE}
# helper function for plotting

#  | Faculty of Information and Computer Science, University of Ljubljana

# wd
#setwd("D:/Projects/Stan/StanCon2018")
#setwd("d:/work/bitbucket/l1-7542_stan/StanCon2018/")


# includes
library(ggplot2)
library(plyr)
library(rstan)
library(cowplot)
library(reshape)

speedup_plot <- function(stan_files) {
	# load data ----
	stan_summary <- NULL
	stan_summary_log <- NULL
	
	for (fn in stan_files) {
		summary <- readRDS(fn)
		
		stan_summary <- rbind(stan_summary, data.frame(N = summary$N,
																									 time = summary$time,
																									 GPU = summary$GPU))
		
		stan_summary_log <- rbind(stan_summary_log, data.frame(N = log(summary$N, 10),
																									 time = log(summary$time, 10),
																									 GPU = summary$GPU))
	}
	
	# get mean and CI ----
	stan_mean <- ddply(.data = stan_summary,
										 .variables = ~ N + GPU,
										 .fun = summarize,
										 time_mean = mean(time),
										 low_time = quantile(time, 0.025, na.rm = TRUE),
										 high_time = quantile(time, 0.975, na.rm = TRUE))
	
	stan_mean_log <- ddply(.data = stan_summary_log,
										 .variables = ~ N + GPU,
										 .fun = summarize,
										 time_mean = mean(time),
										 low_time = quantile(time, 0.025, na.rm = TRUE),
										 high_time = quantile(time, 0.975, na.rm = TRUE))
	
	# plot ----
	left_plot <- ggplot(data = stan_mean,
	                    aes(x = N, y = time_mean, group = GPU, colour = GPU)) +
	  geom_point(data = stan_summary,
	             aes(x = N, y = time, colour = GPU),
	             size = 4, alpha = 0.3, shape = 16) +
		geom_line(size = 1) +
		ylab("Time [s]") +
		xlab("N") +
		theme_minimal() +
		theme(legend.position = "bottom", legend.title = element_blank()) +
		scale_colour_manual(values = c("#fc8d59", "#91bfdb"), labels = c("CPU", "GPU"))
	
	
	right_plot <- ggplot(data = stan_mean_log, 
	                     aes(x = N, y = time_mean, group = GPU, colour = GPU)) +
		geom_point(data = stan_summary_log, aes(x = N, y = time, colour = GPU),
		           size = 4, alpha = 0.3, shape = 16) +
		geom_line(size = 1) +
		ylab(expression(log[10](time)~"[s]")) +
		xlab(expression(log[10](N))) +
		theme_minimal() +
		theme(legend.position = "bottom", legend.title = element_blank()) +
		scale_colour_manual(values = c("#fc8d59", "#91bfdb"), labels = c("CPU", "GPU"))
	
	plot_grid(left_plot, right_plot, ncol = 2, nrow = 1, scale = 0.9)
}

# scripts for data generation
source("./_Simulations/GP_data_generator.R")
source("./_Simulations/GPP_data_generator.R")

# RERUN SIMULATIONS?
# !!! WARNING: UNCOMMENTING THE TWO LINES BELOW RERUNS ALL SIMULATIONS
# !!! THIS MIGHT TAKE SEVERAL HOURS TO COMPLETE
# !!! YOU ALSO NEED TO SET FOLDERS FOR cmdstan and gpustan

# source("./_Simulations/GP.R")
# source("./_Simulations/GPP.R")

```

# Introduction

The Stan Math library's Hamilton Monte Carlo (HMC) sampler has computationally expensive draws while usually searching the target distribution more effectivly than alternative MCMC methods with less iterations. The bottleneck within draws makes Stan a prime candidate for GPU optimizations within samples. This project implements GPU optimizations for the cholesky decompostion and it's derivative in the Stan Math library [@stanmath2015]. Several modeling languages exist which implement MCMC sampling using GPUs, however this is the first known open source implementation of the cholesky decomposition with a GPU in a HMC setting. Furthermore, the GPU kernels are written in OpenCL which allows the methods to be implemented across any type of GPU. While results show that GPU optimizations are not optimal for small $N\times M$ matrices, large matrices can see speedups of 13 fold while retaining the same precision as models run purely on a CPU. 

# GPU implementation

<!--
Currently, we aim at speeding up the biggest computational bottlenecks on the GPU, while the remaining (non-parallelized) Stan code is executed on the CPU. Therefore, we move the data to and from the GPU for each GPU-parallelized function. Removing this often unnecessary data transfer to and from the GPU is one of the main priorities of  future work.
-->
The largest linear algebra bottlenecks in stan tends to come from the cholesky decomposition, it's derivative, and the derivative of solving $Ax = B$. In order to reduce these bottlenecks, the GPU methods implemented in the Stan Math library include:

\begin{enumerate}
\item	Matrix transpose
\item	Multiplication of matrices with a diagonal and scalar
\item	Subtraction of matrices
\item	Copying submatrices
\item	Matrix multiplication
\item	Lower triangular matrix inverse
\item	Cholesky decomposition
\item First derivative of Cholesky Decomposition
\end{enumerate}

Where [what numbers are from [@Cesnovar2017]?] come from [@Cesnovar2017]. Some of the optimizations used here are simple on a GPU. For instance, in multiplication of a $m \times n$ matrix with a scalar, we create $m \times n$ threads, where each thread is assigned a single multiplication. In the next section we will cover how the methods in (6-8) utilize the methods of (1-5).

The OpenCL [@StoneOpenCL2010] context which manages the devices, platforms, memory, and kernels is built into stan using the \texttt{C++11} Meyer's singleton pattern called \texttt{opencl\_context\_base::getInstance()}. Developers are able to access the context through an friend adapter class called \texttt{opencl\_context} which provides a simple wrapper for accessing the base context.

## Matrix multiplication

Our matrix multiplication implementation is based on routines in cuBLAS [@CUBLAS] and clBLAST [@clBLAST]. The matrix multiplication routines are optimized through two standard methods, assigning additional work to threads in large matrix multiplications and the use of tiling in local memory. In addition, optimizations for special cases are created such as $A \times A^T$, since the result is symmetric and the number of multiplications can be reduced by half. 

## Inverting a lower triangular matrix

The mostly widely used CPU algorithms for inverting a lower triangular matrix are not suitable for many-core architectures. Figure \ref{fig:blockInverse} gives a graphical illustration of the solution proposed in [@Mahfoudhi2012] which replaces most of the sequential code with matrix multiplications and is more suited for many-core systems.

The input matrix is split into blocks as shown in Figure \ref{fig:blockInverse}. The first step is to calculate the matrix inversion of the smaller matrices $A1$ and $A2$. These inverses are done using the basic sequential algorithms, with small amounts of parallelism. The final step is the calculation of $C3 = -C2 \times A3 \times C1$. The optimal number of blocks depends on the input matrix size and the GPU used\footnote{Generally thread blocks and warps will grouped in powers of two so the optimal block size is recommended to be a power of two such as 32x32}. 

![\label{fig:blockInverse}Blocked version of the lower triangualar matrix inverse.](_Figures/blockInverse.pdf)

## Cholesky decompostion

The GPU implementation of the Cholesky decompostion is based on the blocked algorithm proposed in [@LouterNool1992]. Similar to the implementation of the lower triangular matrix inverse, the input matrix is split into blocks, as shown in Figure \ref{fig:blockCholesky}. An basic algorithm is first used to calculate the cholesky decomposition of $A_{11}$ and then the inverse of $L_{11}^T$. $L_{21}$ and $L_{22}$ are calculated as follows:

$$L_{21} = A_{21} (L_{11}^T)^{(-1)}$$

$$L_{22} = A_{22} - L_{21} (L_{21})^T$$

For larger matrices $(n > 1000)$, the algorithm is executed in 2 levels. For example, when $n = 2000$, the size of the block $A_{11}$ is $m = 400$. Because the sequential algorithm would be slow for a large $A_{11}$ block, the routine is run recursively on $A_{11}$ until $m$ reaches a reasonable size. 

![\label{fig:blockCholesky}Blocked version of the Cholesky decomposition.](_Figures/blockCholesky.pdf)

The implementation of the derivative of the Cholesky decomposition is based on the blocked version presented in [@Murray2016]. This algorithm is cache friendly and uses GPU-suitable matrix operations. Similar to the inversion and cholesky decomposition, the input matrix is split into smaller blocks on which we then perform various already discussed matrix operations: transpose, multiplication, lower triangular matrix inversion and subtraction. For details on the algorithm, refer to [@Murray2016].

Users can access the Cholesky GPU routines by calling \texttt{cholesky\_decompose\_gpu()} in the stan language. In the future all GPU methods will be implimented in the same way so that users can make their code access the GPU routines through calling \texttt{<func\_name>\_gpu()}.


# Example 1: Gaussian process regression

Models that use large covariance matrices will tend to benefit the most from the cholesky GPU routines. The example below uses 1D GP regression with hyperpriors as exampled in the case study [@Betancourt2017] (see appendix).

This example uses a toy dataset based on a simple functional relationship between $x$ and $y$ with added Gaussian noise:

$$x_i \sim_{\text{iid}} U(-10,10)$$
$$y_i | x_i \sim_{\text{iid}} N \left( f(x), \frac{1}{10} \right), i = 1..n,$$
where $f(x) = \beta(x + x^2 - x^3 + 100 \sin 2x - \alpha)$. Parameters $\beta$ and $\alpha$ were set so that the $E[f] = 0$ and $Var[f] = 1$. Figure \ref{fig:GP_fit} shows that there is no difference between GPU and CPU fits.

```{r echo = FALSE, fig.cap="\\label{fig:GP_fit}Comparison of CPU and GPU fits."}

# load  results ----
stan_files <- list.files(
  path = "./_Simulations/_Output/GP/",
  pattern = "GP", full.names = T
)

for (fn in stan_files) {
  summary <- readRDS(fn)
  
  # take one fit on largest N ----
  if (summary$N == 100 && summary$iteration == 1)
  {

    if (summary$GPU)
      gpu_predict <- data.frame(x = summary$x,
                                y = summary$y,
                                x_predict = summary$x_predict,
                                y_predict = summary$y_predict,
                                GPU = TRUE)
    else
      cpu_predict <- data.frame(x = summary$x,
                                y = summary$y,
                                x_predict = summary$x_predict,
                                y_predict = summary$y_predict,
                                GPU = FALSE)
  }
}

df <- rbind(gpu_predict, cpu_predict)

ggplot() +
  geom_point(data = gpu_predict, aes(x = x, y = y), alpha = 0.3, shape = 16) +
  geom_line(data = df, aes(x = x_predict, y = y_predict, colour = GPU), size = 1) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_colour_manual(values = c("#fc8d59", "#91bfdb"), labels = c("CPU", "GPU"))

```

The model is executed and timing results are stored for multiple input sizes over the GPU and CPU implementation. The measurements include sampling and warmup iterations, but not model compilation time\footnote{The experiments were executed on a desktop computer with a Intel Xeon CPU running at $2.3 GHz$ and a NVIDIA GTX1070 GPU}. Results are shown in Figure \ref{fig:GP_results}. Due to unnecessary data transfers, the GPU implementation is not faster than the CPU version for smaller input sizes ($n < 600$). For larger $n$, the data transfer become negligible and we can observe a speedup of $~13$ for $n = 3000$. Speedup measurements for larger $n$ were infeasible due to large CPU computation times. 

```{r echo = FALSE, fig.cap="\\label{fig:GP_results}Visualizations of speedup when using our GPU approach compared to default CPU implementation."}

# load  results ----
stan_files <- list.files(
	path = "./_Simulations/_Output/GP/",
	pattern = "GP", full.names = T
)

# plot
speedup_plot(stan_files)
```

To estimate the timing of the GPU vs. CPU routines we build a small linear regression on the simulations of the form $time \sim D_{GPU} \times N_{sims} + N_{sims} + \epsilon$ and predict the GPU and CPU times for $N = \{150, 200, 250\}$.


```{r echo = FALSE, warning=FALSE, message=FALSE}
stan_summary <- NULL

for (fn in stan_files) {
	summary <- readRDS(fn)
	stan_summary <- rbind(stan_summary, data.frame(N = summary$N,
																								 time = summary$time,
																								 GPU = summary$GPU))
}

stan_summary$GPU = as.integer(stan_summary$GPU)
time_pred = lm(time ~ GPU:N + N + 0, data = stan_summary)
new_dat = expand.grid(N = c(150, 200, 250), GPU = c(0,1))
new_dat$preds = predict(time_pred, newdata = new_dat)
new_dat = cast(new_dat, N ~ GPU)
colnames(new_dat) = c("N", "CPU", "GPU")
#knitr::kable(new_dat, format = "latex", digits = 2,
#             caption = "The predictions for CPU vs. GPU times for larger N")
```


\begin{table}[!htbp]
\caption{\label{tab:}The predictions for CPU vs. GPU times for larger N}
\centering
\begin{tabular}[t]{r|r|r}
\hline
N & CPU & GPU\\
\hline
150 & 23.65 & 6.34\\
\hline
200 & 31.54 & 8.45\\
\hline
250 & 39.42 & 10.57\\
\hline
\end{tabular}
\end{table}

Futher tests are necessary, but basic intuition and the back of a napkin regression implies that this method will continue 

# Example 2: Spatial Gaussian predictive process

TODO: Description of the model.

Figure \ref{fig:GPP_parameters} shows that there is no difference in fitter parameters between the GPU and CPU approaches.

```{r echo = FALSE, fig.cap="\\label{fig:GPP_parameters}Comparison of calculated parameters between CPU and GPU."}

# load  results ----
stan_files <- list.files(
	path = "./_Simulations/_Output/GPP/",
	pattern = "GPP", full.names = T
)

stan_summary <- NULL
for (fn in stan_files) {
	summary <- readRDS(fn)
	
	if (summary$N == 200)
	{
		stan_summary <- rbind(stan_summary, data.frame(GPU = summary$GPU,
																								Eta = summary$parameters_fit$eta,
																								Sigma = summary$parameters_fit$sigma,
																								Phi = summary$parameters_fit$phi,
																								Beta = summary$parameters_fit$beta
																								))
	}
}

# melt ----
stan_melt <- melt(stan_summary, id=c("GPU"))

# plot ----
ggplot(stan_melt, aes(x = GPU, y = value, fill = GPU)) +
  facet_wrap(~ variable, scales="free", ncol = 4) +
  stat_boxplot(geom ='errorbar') + 
  geom_boxplot() +
  ylab("") +
  xlab("Parameter") +
  theme_minimal() +
  theme(legend.position = "none") +
  theme(panel.spacing.x = unit(4, "lines")) +
  scale_x_discrete(labels=c("CPU", "GPU")) +
  scale_fill_manual(values = c("#fc8d59", "#91bfdb"))
```

TODO: Text.

Results are shown in Figure \ref{fig:GPP_results}.

```{r echo = FALSE, fig.cap="\\label{fig:GPP_results}Visualizations of speedup when using our GPU approach compared to default CPU implementation."}

# load  results ----
stan_files <- list.files(
	path = "./_Simulations/_Output/GPP/",
	pattern = "GPP", full.names = T
)

# plot
speedup_plot(stan_files)
```

# Conclusion

This project shows the GPU routines in stan can give both practical and powerful speedups. Parallelizing the cholesky and it's derivative can give 13 fold speedups or more for programs which depend on large covariance matrices. As this project continues, future areas of reasearch include (a) removing unnecessary data transfers to and from the GPU, which is currently our largest bottleneck, (b) allowing \texttt{rstan} [@rstan2018] to access the GPU methods, and (c) adding GPU-parallel implementations for other computational building blocks such as other matrix methods, density calculations, and generating random variates.

\newpage
# Appendix

## Stan model for Gaussian process regression

```
functions {
  vector gp_pred_rng(real[] x2,
                     vector y1, real[] x1,
                     real alpha, real rho, real sigma, real delta) {
    int N1 = rows(y1);
    int N2 = size(x2);
    vector[N2] f2;
    {
      matrix[N1, N1] K =   cov_exp_quad(x1, alpha, rho)
                         + diag_matrix(rep_vector(square(sigma), N1));
      matrix[N1, N1] L_K = cholesky_decompose(K);

      vector[N1] L_K_div_y1 = mdivide_left_tri_low(L_K, y1);
      vector[N1] K_div_y1 = mdivide_right_tri_low(L_K_div_y1', L_K)';
      matrix[N1, N2] k_x1_x2 = cov_exp_quad(x1, x2, alpha, rho);
      vector[N2] f2_mu = (k_x1_x2' * K_div_y1);
      matrix[N1, N2] v_pred = mdivide_left_tri_low(L_K, k_x1_x2);
      matrix[N2, N2] cov_f2 =   cov_exp_quad(x2, alpha, rho) - v_pred' * v_pred
                              + diag_matrix(rep_vector(delta, N2));
      f2 = multi_normal_rng(f2_mu, cov_f2);
    }
    return f2;
  }
}

data {
  int<lower=1> N;
  real x[N];
  vector[N] y;

  int<lower=1> N_predict;
  real x_predict[N_predict];
}

parameters {
  real<lower=0> rho;
  real<lower=0> alpha;
  real<lower=0> sigma;
}

model {
  matrix[N, N] cov =   cov_exp_quad(x, alpha, rho)
                     + diag_matrix(rep_vector(square(sigma), N));
  matrix[N, N] L_cov = cholesky_decompose(cov);

  // P[rho < 2.0] = 0.01
  // P[rho > 10] = 0.01
  rho ~ inv_gamma(8.91924, 34.5805);
  alpha ~ normal(0, 2);
  sigma ~ normal(0, 1);

  y ~ multi_normal_cholesky(rep_vector(0, N), L_cov);
}

generated quantities {
  vector[N_predict] f_predict = gp_pred_rng(x_predict, y, x, alpha, rho, sigma, 1e-10);
  vector[N_predict] y_predict;
  for (n in 1:N_predict)
    y_predict[n] = normal_rng(f_predict[n], sigma);
}

```

## Stan model for spatial Gaussian predictive process

```
data {
  int<lower = 1> N;
  int<lower = 1> p;
  matrix[N, p] X;
  int<lower = 0> y[N];
  matrix[N, N] D;
}

parameters {
  vector[N] z;
  real<lower=0> eta;
  real<lower=0> phi;
  real<lower=0> sigma;
  vector[p] beta;
}

transformed parameters {
  cov_matrix[N] Sigma;
  vector[N] w;
  real<lower = 0> eta_sq;
  real<lower = 0> sig_sq;
  
  eta_sq = pow(eta, 2);
  sig_sq = pow(sigma, 2);
  
  for (i in 1:(N - 1)) {
    for (j in (i + 1):N) {
      Sigma[i, j] = eta_sq * exp(-D[i, j] * phi);
      Sigma[j, i] = Sigma[i, j];
    }
  }

  for (k in 1:N) Sigma[k, k] = eta_sq + sig_sq;
  w = cholesky_decompose(Sigma) * z;
}

model {
  eta ~ normal(0, 1);
  sigma ~ normal(0, 1);
  phi ~ normal(0, 5);
  beta ~ normal(0, 1);
  z ~ normal(0, 1);
  y ~ poisson_log(X * beta + w);
}
```
## Original Computing Environment

```{r}
sessionInfo()
```

\newpage
# References